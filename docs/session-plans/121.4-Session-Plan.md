# Session 121.4: Data Sync

**Parent Plan**: `121-Session-Plan.md`
**Prerequisite**: Sessions 121.1, 121.2, and 121.3 completed
**Target**: ~40k tokens

---

## Scope

Binary SQLite files (`data.db`) cannot be merged by git. When two developers modify work items independently, git produces an unresolvable binary conflict. This session implements the **SQLite + JSON sync pattern**:

- **SQLite** (`data.db`) — fast local queries, gitignored
- **JSON** (`data.json`) — git-tracked export, human-readable, mergeable

**What gets created:**
- `pm sync export` command — dumps all tables to `data.json`
- `pm sync import` command — loads `data.json` into `data.db` (with merge/replace modes)
- Git hook templates — auto-export on commit, auto-import on pull

**What gets modified:**
- `.pm/.gitignore` — ignore `data.db`, track `data.json`
- CLI commands/main — add `Sync` variant and handlers

### Concepts Taught

- **JSON as sync format** — Why binary SQLite can't be merged, and JSON export solves this
- **Upsert with conflict resolution** — Timestamp-based "last write wins" using `updated_at`
- **UUID-based identity** — No ID collisions between disconnected developers
- **Git hooks** — Pre-commit, post-merge, post-checkout automation

---

## Prerequisites Check

Verify Sessions 121.1-121.3 are complete:

```bash
just check-backend        # Should pass
just test-backend         # Should pass
just check-rs-cli         # Should pass
```

---

## Implementation Order

### Step 1: Fix `.pm/.gitignore` — Track JSON, Ignore SQLite

**The problem**: Session 121.1 added `!data.db` to force-track the binary SQLite database. This breaks when multiple developers work in parallel — git cannot merge binary files.

**The fix**: Ignore `data.db` (local performance), track `data.json` (git-friendly sync format).

**File**: `.pm/.gitignore`

**Find `!data.db`** (currently negates the root `.gitignore` `*.db` pattern). Replace the entire file:

```gitignore
# .pm/.gitignore
# ===============
# Runtime files that should NOT be committed.
# data.json and config.toml ARE tracked (not listed here = tracked).

# SQLite database (local performance — use data.json for git sync)
data.db

# Negate root .gitignore's *.json pattern if one exists
!data.json

# Installed binaries (downloaded via install.sh or built locally)
bin/

# SQLite WAL/SHM runtime files (recreated automatically on server start)
*.db-wal
*.db-shm

# Server discovery file (ephemeral — changes every server start)
server.json

# Process lock file (prevents duplicate server instances)
server.lock

# Server log files
logs/
log/

# Tauri desktop runtime data
tauri/
```

**After editing, un-track data.db if it was previously tracked:**
```bash
git rm --cached .pm/data.db 2>/dev/null || true
```

**Verification**:
```bash
git status
# .pm/data.db should NOT appear (gitignored)
# .pm/data.json should appear when created (tracked)
```

---

### Step 2: Define the Export Schema

**Concept: JSON as the Git-Friendly Format**

JSON is human-readable, diff-able, and git can show meaningful diffs and even three-way merge it in many cases. The export format is a single JSON file containing all project management data, versioned so future schema changes can be handled gracefully.

**The export schema** (this is the contract — `data.json` always follows this shape):

```json
{
  "schema_version": 1,
  "exported_at": "2026-02-08T12:00:00Z",
  "exported_by": "pm-cli 0.1.0",
  "projects": [
    {
      "id": "uuid",
      "title": "V2 Redesign",
      "description": "...",
      "status": "active",
      "created_at": "2026-01-01T00:00:00Z",
      "updated_at": "2026-02-01T00:00:00Z",
      "created_by": "user-uuid",
      "updated_by": "user-uuid"
    }
  ],
  "work_items": [
    {
      "id": "uuid",
      "project_id": "uuid",
      "parent_id": null,
      "item_type": "story",
      "title": "Homepage redesign",
      "description": "...",
      "status": "in_progress",
      "priority": "high",
      "position": 1,
      "story_points": 5,
      "assignee_id": null,
      "sprint_id": null,
      "version": 3,
      "created_at": "2026-01-15T00:00:00Z",
      "updated_at": "2026-02-05T10:30:00Z",
      "created_by": "user-uuid",
      "updated_by": "user-uuid",
      "deleted_at": null
    }
  ],
  "sprints": [],
  "comments": [],
  "swim_lanes": []
}
```

**Key design decisions:**
- `schema_version` enables forward-compatible imports
- All entities include `updated_at` for conflict resolution
- `deleted_at` is included for soft-delete awareness during import
- UUIDs are strings (consistent with SQLite TEXT storage)

This step is teaching-only — no code to write yet. The struct definitions come in Step 3.

---

### Step 3: Add `Sync` Variant to CLI Commands

**File**: `backend/crates/pm-cli/src/commands.rs`

**Find the `Commands` enum** (currently has `Project`, `WorkItem`, `Comment`, `Desktop`). Add after `Desktop`:

```rust
    /// Sync agile board data (export/import for git collaboration)
    Sync {
        #[command(subcommand)]
        action: SyncCommands,
    },
```

**Add `SyncCommands` enum** in a new `sync_commands.rs` (matching the existing flat file pattern: `project_commands.rs`, `work_item_commands.rs`, `comment_commands.rs`):

```rust
#[derive(Subcommand)]
pub(crate) enum SyncCommands {
    /// Export database to JSON for git tracking
    Export {
        /// Output file path (default: .pm/data.json)
        #[arg(short, long)]
        output: Option<String>,

        /// Suppress output except errors
        #[arg(long)]
        quiet: bool,
    },

    /// Import JSON data into local database
    Import {
        /// Input file path (default: .pm/data.json)
        file: Option<String>,

        /// Merge with existing data (default: replace)
        #[arg(long)]
        merge: bool,

        /// Suppress output except errors
        #[arg(long)]
        quiet: bool,
    },
}
```

**Verification**: `just check-rs-cli`

---

### Step 4: Implement `pm sync export`

**File**: `backend/crates/pm-cli/src/sync_commands.rs` (NEW — flat file at src root, matching `project_commands.rs` pattern)

This command connects to the running pm-server, fetches all data, and writes it as JSON.

**Add `mod sync_commands;`** to `main.rs` alongside the existing module declarations:

```rust
// Find this block in main.rs:
mod cli;
mod client;
mod commands;
mod comment_commands;
mod project_commands;
mod work_item_commands;

// Add after work_item_commands:
mod sync_commands;
```

**Architectural Decision: Use Option A (Iterate Existing Client)**

The CLI's `pm_cli::Client` is already wired up in `main.rs` and routes the existing Project/WorkItem/Comment commands through it. The Client has the per-entity methods needed: `list_projects()`, `list_work_items(project_id, ...)`, `list_comments(work_item_id)`. Export iterates: list all projects → for each project, list work items → for each work item, list comments.

**Tradeoffs:**
- No server-side changes needed
- Client already functional (wired up in main.rs)
- N+1 queries — acceptable for the typical project count (1-5 projects per repo)

**Alternative approaches** (not used in this plan):
- **Option B**: Add `GET /api/export` and `POST /api/import` bulk endpoints in `pm-api` — requires server-side work, a separate session if performance matters
- **Option C**: Direct DB access via sqlx — couples CLI to DB schema, defeats the purpose of the server layer

**Server must be running**: Export and import both go through the HTTP Client, which requires a running pm-server. If the server is not running, the Client will fail with a connection error. The CLI's existing `discover_server_url()` already handles this with a clear error message ("No running pm-server found").

```rust
use std::path::PathBuf;

use serde::{Deserialize, Serialize};

/// The export format — everything in data.json
#[derive(Debug, Serialize, Deserialize)]
pub struct ExportData {
    pub schema_version: u32,
    pub exported_at: String,
    pub exported_by: String,
    pub projects: Vec<serde_json::Value>,
    pub work_items: Vec<serde_json::Value>,
    pub sprints: Vec<serde_json::Value>,
    pub comments: Vec<serde_json::Value>,
    pub swim_lanes: Vec<serde_json::Value>,
}

/// Export all data from the running server to a JSON file.
///
/// Requires a running pm-server (export goes through the HTTP API).
/// If server is not running, discover_server_url() in main.rs will
/// exit with a clear error message before this function is called.
pub async fn export(
    client: &pm_cli::Client,
    output: Option<String>,
    quiet: bool,
) -> Result<(), Box<dyn std::error::Error>> {
    let config_dir = pm_config::Config::config_dir()?;
    let output_path = match output {
        Some(p) => PathBuf::from(p),
        None => config_dir.join("data.json"),
    };

    if !quiet {
        eprintln!("Exporting to {}...", output_path.display());
    }

    // Fetch all data via existing Client methods (Option A: iterate per-project)
    let projects_response = client.list_projects().await?;

    // Parse project IDs from the response to iterate work items
    let project_ids: Vec<String> = projects_response
        .as_array()
        .unwrap_or(&vec![])
        .iter()
        .filter_map(|p| p.get("id").and_then(|v| v.as_str()).map(String::from))
        .collect();

    let mut all_work_items = Vec::new();
    let mut all_comments = Vec::new();

    for project_id in &project_ids {
        // Fetch work items for this project
        let items = client.list_work_items(project_id, None, None).await?;
        if let Some(items_arr) = items.as_array() {
            for item in items_arr {
                // Fetch comments for each work item
                if let Some(item_id) = item.get("id").and_then(|v| v.as_str()) {
                    let comments = client.list_comments(item_id).await?;
                    if let Some(comments_arr) = comments.as_array() {
                        all_comments.extend(comments_arr.clone());
                    }
                }
                all_work_items.push(item.clone());
            }
        }
    }

    let export = ExportData {
        schema_version: 1,
        exported_at: chrono::Utc::now().to_rfc3339(),
        exported_by: format!("pm-cli {}", env!("CARGO_PKG_VERSION")),
        projects: projects_response.as_array().cloned().unwrap_or_default(),
        work_items: all_work_items,
        sprints: vec![], // Sprint listing not yet in Client — add when available
        comments: all_comments,
        swim_lanes: vec![], // Swim lane listing not yet in Client — add when available
    };

    let json = serde_json::to_string_pretty(&export)?;
    std::fs::write(&output_path, &json)?;

    if !quiet {
        eprintln!("Exported {} projects, {} work items, {} comments",
            export.projects.len(),
            export.work_items.len(),
            export.comments.len(),
        );
        eprintln!("Written to: {}", output_path.display());
    }

    Ok(())
}
```

**Verification**: `just check-rs-cli`

---

### Step 5: Implement `pm sync import`

The import command reads `data.json` and writes it into the local database. Two modes:

- **Replace** (default): Wipe all tables, insert everything from JSON
- **Merge** (`--merge`): Upsert using timestamp-based "last write wins"

**Server must be running**: Like export, import goes through the HTTP Client. The CLI's `discover_server_url()` handles the missing-server case before this function runs.

**Import via existing Client methods**: The Client already has `create_work_item()` and `update_work_item()`. For replace mode: delete all → create all. For merge mode: get each entity by ID → compare `updated_at` → create or update as needed.

```rust
/// Import data from a JSON file into the local database.
///
/// Requires a running pm-server (import goes through the HTTP API).
pub async fn import(
    client: &pm_cli::Client,
    file: Option<String>,
    merge: bool,
    quiet: bool,
) -> Result<(), Box<dyn std::error::Error>> {
    let config_dir = pm_config::Config::config_dir()?;
    let input_path = match file {
        Some(p) => PathBuf::from(p),
        None => config_dir.join("data.json"),
    };

    if !input_path.exists() {
        return Err(format!("File not found: {}", input_path.display()).into());
    }

    if !quiet {
        eprintln!("Importing from {}...", input_path.display());
    }

    let json = std::fs::read_to_string(&input_path)
        .map_err(|e| format!("Failed to read {}: {}", input_path.display(), e))?;

    let data: ExportData = serde_json::from_str(&json)
        .map_err(|e| format!("Invalid JSON in {}: {}", input_path.display(), e))?;

    if data.schema_version > 1 {
        return Err(format!(
            "Unsupported schema version: {} (this CLI supports up to 1). \
             Update pm-cli to import this file.",
            data.schema_version
        ).into());
    }

    let mut stats = ImportStats::default();

    if merge {
        if !quiet {
            eprintln!("Merge mode: keeping newer records by updated_at timestamp");
        }
        import_merge(client, &data, &mut stats).await?;
    } else {
        if !quiet {
            eprintln!("Replace mode: clearing existing data and inserting all");
        }
        import_replace(client, &data, &mut stats).await?;
    }

    if !quiet {
        eprintln!("Import complete: {} created, {} updated, {} skipped (local newer)",
            stats.created, stats.updated, stats.skipped);
    }

    Ok(())
}

#[derive(Default)]
struct ImportStats {
    created: usize,
    updated: usize,
    skipped: usize,
}
```

**Conflict resolution strategy:**

| Scenario | Action |
|----------|--------|
| Record exists locally, import has newer `updated_at` | Upsert (import wins) |
| Record exists locally, local has newer `updated_at` | Skip (local wins) |
| Record only in import | Insert |
| Record only locally (not in import), merge mode | Keep (don't delete local-only records) |
| Record only locally, replace mode | Deleted (table was wiped) |

**`import_merge()` pseudocode:**
```rust
async fn import_merge(client: &Client, data: &ExportData, stats: &mut ImportStats) -> Result<...> {
    for project in &data.projects {
        let id = project["id"].as_str().unwrap();
        match client.get_project(id).await {
            Ok(local) => {
                let local_updated = local["updated_at"].as_str().unwrap_or("");
                let import_updated = project["updated_at"].as_str().unwrap_or("");
                if import_updated > local_updated {
                    // Import is newer → update
                    client.update_project(id, project).await?;
                    stats.updated += 1;
                } else {
                    stats.skipped += 1;
                }
            }
            Err(_) => {
                // Not found locally → create
                client.create_project(project).await?;
                stats.created += 1;
            }
        }
    }
    // Repeat for work_items, comments, etc.
    Ok(())
}
```

**Verification**: `just check-rs-cli`

---

### Step 6: Wire Sync Commands into main.rs

**File**: `backend/crates/pm-cli/src/main.rs`

**Find the `match cli.command` block** (currently has arms for `Project`, `WorkItem`, `Comment`). Add the `Sync` arm:

**Also add the import** at the top of main.rs, alongside existing command imports:
```rust
use crate::sync_commands::SyncCommands;
```

```rust
        Commands::Sync { action } => match action {
            SyncCommands::Export { output, quiet } => {
                match sync_commands::export(&client, output, quiet).await {
                    Ok(()) => ExitCode::SUCCESS,
                    Err(e) => {
                        eprintln!("Export error: {}", e);
                        ExitCode::FAILURE
                    }
                }
            }
            SyncCommands::Import { file, merge, quiet } => {
                match sync_commands::import(&client, file, merge, quiet).await {
                    Ok(()) => ExitCode::SUCCESS,
                    Err(e) => {
                        eprintln!("Import error: {}", e);
                        ExitCode::FAILURE
                    }
                }
            }
        },
```

**Note**: Both `sync export` and `sync import` require a running server (they go through the HTTP Client). Unlike `Desktop` (which is intercepted before server discovery in main.rs), `Sync` commands must go through the normal server discovery path. The existing `discover_server_url()` will print a clear error if no server is running.

**Important**: `Sync` goes AFTER the `client` creation in main.rs (it needs the client), unlike `Desktop` which goes BEFORE (it doesn't need a server).

**Verification**:
```bash
just check-rs-cli
cargo run -p pm-cli -- sync --help
# Should show "export" and "import" subcommands
```

---

### Step 7: Document Git Hook Templates

**Concept: Git Hooks for Automated Sync**

Git hooks are scripts that run automatically at certain points in the git workflow. By adding hooks that auto-export before commits and auto-import after pulls, the team never has to remember to sync manually.

These hooks are documented but NOT auto-installed — the user runs `pm sync init` or copies them manually. This is a deliberate choice: auto-installing git hooks without user consent is poor practice.

**Pre-commit hook** — auto-export before every commit:

```bash
#!/bin/bash
# .git/hooks/pre-commit
# Auto-export agile board data before commit

if command -v pm &>/dev/null && [ -f .pm/data.db ]; then
    pm sync export --quiet
    git add .pm/data.json
fi
```

**Post-merge hook** — auto-import after every pull/merge:

```bash
#!/bin/bash
# .git/hooks/post-merge
# Auto-import agile board data after pull

if command -v pm &>/dev/null && [ -f .pm/data.json ]; then
    pm sync import --merge --quiet
fi
```

**Post-checkout hook** — auto-import on branch switch or clone:

```bash
#!/bin/bash
# .git/hooks/post-checkout
# Auto-import agile board data on branch switch

# Only run for branch checkouts, not file checkouts
if [ "$3" = "1" ] && command -v pm &>/dev/null && [ -f .pm/data.json ]; then
    pm sync import --merge --quiet
fi
```

**Optional: `pm sync init` command** to install the hooks:

This could be added as a convenience, but is NOT required for this session. The hooks are simple enough to copy-paste.

**Verification**: Documentation review only — no code to compile for this step.

---

## Session 121.4 Completion Checklist

```bash
# Full verification sequence
just check-backend        # All Rust code compiles
just clippy-backend       # No clippy warnings
just test-backend         # All tests pass

# CLI shows sync commands
cargo run -p pm-cli -- sync --help
# Should show "export" and "import"

# Verify .gitignore change
git check-ignore .pm/data.db
# Should show: .pm/data.db (ignored)

git check-ignore .pm/data.json
# Should show nothing (NOT ignored = tracked)

# Verify no data.db in git
git ls-files .pm/data.db
# Should return nothing
```

### Files Created (1)

| File | Purpose |
|------|---------|
| `backend/crates/pm-cli/src/sync_commands.rs` | `pm sync export` and `pm sync import` (flat file at src root, matching existing pattern) |

### Files Modified (3)

| File | Change |
|------|--------|
| `.pm/.gitignore` | Track `data.json`, ignore `data.db` |
| `backend/crates/pm-cli/src/commands.rs` | Add `Sync` variant and `SyncCommands` enum |
| `backend/crates/pm-cli/src/main.rs` | Wire sync commands into main match |

---

## Complete Developer Workflow

After all Session 121 sub-sessions (121.1-121.4) are complete:

### Developer A: Initialize

```bash
cd my-web-app   # Any git repo, any language
pm-server &
pm project create "V2 Redesign"
pm work-item create --project-id <uuid> --type story --title "Homepage"

# Export (manual, or automatic with pre-commit hook)
pm sync export
git add .pm/data.json .pm/config.toml
git commit -m "Add V2 project"
git push
```

### Developer B: Clone and Collaborate

```bash
git clone my-web-app && cd my-web-app
curl -fsSL .../install.sh | bash    # Install tools to .pm/bin/
pm sync import                       # Creates data.db from data.json
pm-server &
pm work-item list                    # Shows Developer A's work items
pm work-item create --project-id <uuid> --type story --title "Navbar"
pm sync export
git commit -am "Add navbar task" && git push
```

### Developer A: Pull Updates

```bash
git pull                            # Gets updated data.json
pm sync import --merge              # Merges new data (last-write-wins)
pm work-item list                   # Shows: Homepage, Navbar
```

### With Git Hooks (Fully Automated)

```bash
# Install hooks once:
cp docs/hooks/pre-commit .git/hooks/
cp docs/hooks/post-merge .git/hooks/
chmod +x .git/hooks/*

# Then just use git normally:
pm work-item create ...
git commit -am "My changes"        # Pre-commit hook auto-exports
git pull                            # Post-merge hook auto-imports
```

---

## Implementation Notes

**Approach: Option A (iterate existing Client)**. The CLI's `pm_cli::Client` is already wired up in `main.rs` with the per-entity methods needed: `list_projects()`, `list_work_items()`, `list_comments()`, `create_work_item()`, `update_work_item()`, `delete_work_item()`. Export iterates projects→items→comments. Import uses create/update methods. No server-side changes needed.

**Future optimization (Option B)**: If the iteration approach becomes too slow for large datasets, add `GET /api/export` and `POST /api/import` bulk endpoints in `pm-api`. This would be a separate session.

**Server must be running**: Both export and import require a running pm-server. The CLI's existing `discover_server_url()` function in `main.rs` handles the missing-server case with a clear error message and exits before the sync commands run. No additional server-not-running handling is needed in the sync module itself.

**Missing Client methods**: The Client currently lacks sprint listing and swim lane listing. The export marks these as empty arrays with comments. When those Client methods are added (sessions 40+), update the export to include them.

**Dependencies**: This session needs `chrono` (for timestamps) and `serde_json` (for JSON) in pm-cli — both are already workspace dependencies.

---

## Next Steps

After Session 121 is fully complete (121.1-121.4), the project has:
- Portable `.pm/` directory shared by all tools (CLI, server, desktop)
- Zero env vars for discovery (git → config.json → global fallback)
- Release distribution with install scripts
- Data sync via JSON export for git collaboration
