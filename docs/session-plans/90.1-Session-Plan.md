# Session 90.1: Foundation (Database Migration + Protobuf)

**Parent Plan**: `90-Session-Plan.md`
**Target**: ~25k tokens
**Prerequisites**: `just check` passes, existing database accessible

---

## ⚠️ CRITICAL: SQLite Foreign Key Preservation

**This migration drops and recreates `pm_work_items`.** In SQLite, when you drop a table, **ALL foreign keys in OTHER tables pointing to it are PERMANENTLY DESTROYED.**

**Tables affected:**
- `pm_comments.work_item_id → pm_work_items(id)` ❌ **LOST without proper handling**
- `pm_time_entries.work_item_id → pm_work_items(id)` ❌ **LOST without proper handling**
- `pm_dependencies.blocking_item_id → pm_work_items(id)` ❌ **LOST without proper handling**
- `pm_dependencies.blocked_item_id → pm_work_items(id)` ❌ **LOST without proper handling**

**The Fix:**
This migration follows the pattern from `20260121000001_create_projects_table.sql`:
1. Save data from **ALL 4 affected tables** (not just pm_work_items)
2. Drop **dependent tables FIRST** (pm_comments, pm_time_entries, pm_dependencies)
3. Drop pm_work_items
4. Recreate pm_work_items with new schema
5. **Recreate all dependent tables WITH FK constraints**
6. Restore ALL data
7. Recreate ALL indexes

**Why this matters:**
- Without this pattern: FKs are silently destroyed, database integrity lost forever
- With this pattern: FKs are preserved, data integrity maintained
- **Automated tests verify FK preservation** to catch regressions

---

**CRITICAL FIXES APPLIED** (from Gordon Ramsay review):
- ✅ **FK preservation pattern** (recreates dependent tables with constraints)
- ✅ Wrapped migration in transaction with error handling
- ✅ Created rollback script for migration reversal (also FK-safe!)
- ✅ Added automated test verification including **FK preservation tests**
- ✅ Fixed protobuf optional field semantics (added `update_parent` field)

---

## Scope

This session establishes the foundation for all three features:

1. **Database Migration** - Add `next_work_item_number` to projects, `item_number` to work items (with FK preservation)
2. **Rollback Script** - Create rollback migration for safety (also FK-safe)
3. **Protobuf Schema** - Add fields for JIRA-style IDs and parent editing (with proper optional semantics)
4. **Run Migration** - Apply changes before Rust compilation (SQLx compile-time checking)
5. **Automated Tests** - Add migration verification tests (including FK preservation)

---

## Implementation Order

### Step 1: Create Database Migration File

**Create**: `backend/crates/pm-db/migrations/20260203000001_add_work_item_numbers.sql`

This migration must handle SQLite's limitation of not allowing `ALTER TABLE ... ADD COLUMN ... NOT NULL` without a default. We use table recreation for `pm_work_items`.

**CRITICAL: This migration is now wrapped in a transaction for atomicity.**

```sql
-- ============================================================
-- Migration: Add work item numbers (JIRA-style IDs)
-- Adds: pm_projects.next_work_item_number
-- Adds: pm_work_items.item_number (unique per project)
--
-- SAFETY: This migration is wrapped in a transaction.
-- If any step fails, the entire migration rolls back.
-- BACKUP YOUR DATABASE BEFORE RUNNING THIS MIGRATION.
--
-- CRITICAL FK PRESERVATION:
-- When dropping pm_work_items, we MUST also recreate all tables
-- that have FKs pointing to it (pm_comments, pm_time_entries, pm_dependencies).
-- Otherwise, those FK constraints are PERMANENTLY LOST (SQLite limitation).
-- ============================================================

BEGIN TRANSACTION;

PRAGMA foreign_keys = OFF;

-- Step 1: Add next_work_item_number to pm_projects
-- SQLite allows ADD COLUMN with DEFAULT, so this is safe
ALTER TABLE pm_projects ADD COLUMN next_work_item_number INTEGER NOT NULL DEFAULT 1;

-- Step 2: Save data from ALL affected tables (not just pm_work_items!)
CREATE TEMPORARY TABLE temp_work_items AS
SELECT
    wi.*,
    ROW_NUMBER() OVER (PARTITION BY wi.project_id ORDER BY wi.created_at, wi.id) as item_number
FROM pm_work_items wi;

CREATE TEMPORARY TABLE temp_comments AS SELECT * FROM pm_comments;
CREATE TEMPORARY TABLE temp_time_entries AS SELECT * FROM pm_time_entries;
CREATE TEMPORARY TABLE temp_dependencies AS SELECT * FROM pm_dependencies;

-- Step 3: Drop dependent tables FIRST (before pm_work_items)
-- This is CRITICAL - if we drop pm_work_items first, FK constraints in these tables are lost
DROP TABLE pm_comments;
DROP TABLE pm_time_entries;
DROP TABLE pm_dependencies;

-- Step 4: Drop pm_work_items
DROP TABLE pm_work_items;

-- Step 5: Recreate pm_work_items with item_number column (NOT NULL)
CREATE TABLE pm_work_items (
    id TEXT PRIMARY KEY,
    item_type TEXT NOT NULL CHECK(item_type IN ('epic', 'story', 'task')),
    parent_id TEXT,
    project_id TEXT NOT NULL,
    position INTEGER NOT NULL DEFAULT 0,
    title TEXT NOT NULL,
    description TEXT,
    status TEXT NOT NULL DEFAULT 'backlog' CHECK(status IN ('backlog', 'todo', 'in_progress', 'review', 'done', 'blocked')),
    priority TEXT NOT NULL DEFAULT 'medium' CHECK(priority IN ('critical', 'high', 'medium', 'low')),
    story_points INTEGER,
    assignee_id TEXT,
    sprint_id TEXT,
    item_number INTEGER NOT NULL,  -- NEW: Project-scoped sequential number
    version INTEGER NOT NULL DEFAULT 1,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    updated_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (project_id) REFERENCES pm_projects(id) ON DELETE CASCADE,
    FOREIGN KEY (parent_id) REFERENCES pm_work_items(id) ON DELETE SET NULL,
    FOREIGN KEY (sprint_id) REFERENCES pm_sprints(id) ON DELETE SET NULL,
    FOREIGN KEY (assignee_id) REFERENCES users(id) ON DELETE SET NULL,
    UNIQUE(project_id, item_number)  -- Enforce uniqueness within project
);

-- Step 6: Recreate pm_comments WITH FK constraint (CRITICAL!)
CREATE TABLE pm_comments (
    id TEXT PRIMARY KEY,
    work_item_id TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    updated_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (work_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (created_by) REFERENCES users(id),
    FOREIGN KEY (updated_by) REFERENCES users(id)
);

-- Step 7: Recreate pm_time_entries WITH FK constraint (CRITICAL!)
CREATE TABLE pm_time_entries (
    id TEXT PRIMARY KEY,
    work_item_id TEXT NOT NULL,
    user_id TEXT NOT NULL,
    started_at INTEGER NOT NULL,
    ended_at INTEGER,
    duration_seconds INTEGER,
    description TEXT,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (work_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);

-- Step 8: Recreate pm_dependencies WITH FK constraints (CRITICAL!)
CREATE TABLE pm_dependencies (
    id TEXT PRIMARY KEY,
    blocking_item_id TEXT NOT NULL,
    blocked_item_id TEXT NOT NULL,
    dependency_type TEXT NOT NULL DEFAULT 'blocks' CHECK(dependency_type IN ('blocks', 'relates_to')),
    created_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (blocking_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (blocked_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (created_by) REFERENCES users(id),
    UNIQUE(blocking_item_id, blocked_item_id),
    CHECK(blocking_item_id != blocked_item_id)
);

-- Step 9: Restore ALL data
INSERT INTO pm_work_items (
    id, item_type, parent_id, project_id, position, title, description,
    status, priority, story_points, assignee_id, sprint_id, item_number,
    version, created_at, updated_at, created_by, updated_by, deleted_at
)
SELECT
    id, item_type, parent_id, project_id, position, title, description,
    status, priority, story_points, assignee_id, sprint_id, item_number,
    version, created_at, updated_at, created_by, updated_by, deleted_at
FROM temp_work_items;

INSERT INTO pm_comments SELECT * FROM temp_comments;
INSERT INTO pm_time_entries SELECT * FROM temp_time_entries;
INSERT INTO pm_dependencies SELECT * FROM temp_dependencies;

-- Step 10: Update project counters to max item_number + 1
UPDATE pm_projects SET next_work_item_number = COALESCE(
    (SELECT MAX(item_number) + 1 FROM pm_work_items WHERE project_id = pm_projects.id),
    1
);

-- Step 11: Recreate ALL indexes
-- pm_work_items indexes
CREATE INDEX idx_pm_work_items_project ON pm_work_items(project_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_parent ON pm_work_items(parent_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_sprint ON pm_work_items(sprint_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_status ON pm_work_items(status) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_assignee ON pm_work_items(assignee_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_type ON pm_work_items(item_type) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_item_number ON pm_work_items(project_id, item_number) WHERE deleted_at IS NULL;

-- pm_comments indexes
CREATE INDEX idx_pm_comments_work_item ON pm_comments(work_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_comments_created ON pm_comments(created_at DESC) WHERE deleted_at IS NULL;

-- pm_time_entries indexes
CREATE INDEX idx_pm_time_entries_work_item ON pm_time_entries(work_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_user ON pm_time_entries(user_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_started ON pm_time_entries(started_at DESC) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_running ON pm_time_entries(ended_at) WHERE ended_at IS NULL AND deleted_at IS NULL;

-- pm_dependencies indexes
CREATE INDEX idx_pm_dependencies_blocking ON pm_dependencies(blocking_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_dependencies_blocked ON pm_dependencies(blocked_item_id) WHERE deleted_at IS NULL;

-- Step 12: Cleanup temp tables
DROP TABLE temp_work_items;
DROP TABLE temp_comments;
DROP TABLE temp_time_entries;
DROP TABLE temp_dependencies;

PRAGMA foreign_keys = ON;

-- Commit transaction (rolls back automatically on any error)
COMMIT;
```

**Key Points:**
- **Transaction wrapper** ensures atomicity - if any step fails, entire migration rolls back
- `PRAGMA foreign_keys = OFF/ON` contained within transaction
- `ROW_NUMBER() OVER (PARTITION BY project_id ORDER BY created_at, id)` backfills existing items
- The `UNIQUE(project_id, item_number)` constraint ensures no duplicates within a project
- Index on `(project_id, item_number)` for efficient lookups

**Verification**: File exists at correct path

---

### Step 1b: Create Rollback Migration (NEW - SAFETY)

**Create**: `backend/crates/pm-db/migrations/20260203000001_rollback_add_work_item_numbers.sql`

This rollback script reverses the migration if needed.

```sql
-- ============================================================
-- ROLLBACK Migration: Remove work item numbers
-- Reverses: 20260203000001_add_work_item_numbers.sql
--
-- WARNING: This drops the item_number column, losing the data.
-- Only use this if absolutely necessary.
--
-- CRITICAL FK PRESERVATION:
-- Must recreate dependent tables (pm_comments, pm_time_entries, pm_dependencies)
-- with FK constraints, just like the forward migration.
-- ============================================================

BEGIN TRANSACTION;

PRAGMA foreign_keys = OFF;

-- Step 1: Save data from ALL affected tables
CREATE TEMPORARY TABLE temp_work_items AS
SELECT
    id, item_type, parent_id, project_id, position,
    title, description, status, priority, story_points,
    assignee_id, sprint_id, version,
    created_at, updated_at, created_by, updated_by, deleted_at
FROM pm_work_items;

CREATE TEMPORARY TABLE temp_comments AS SELECT * FROM pm_comments;
CREATE TEMPORARY TABLE temp_time_entries AS SELECT * FROM pm_time_entries;
CREATE TEMPORARY TABLE temp_dependencies AS SELECT * FROM pm_dependencies;

-- Step 2: Drop dependent tables FIRST
DROP TABLE pm_comments;
DROP TABLE pm_time_entries;
DROP TABLE pm_dependencies;

-- Step 3: Drop pm_work_items
DROP TABLE pm_work_items;

-- Step 4: Recreate pm_work_items WITHOUT item_number
CREATE TABLE pm_work_items (
    id TEXT PRIMARY KEY,
    item_type TEXT NOT NULL CHECK(item_type IN ('epic', 'story', 'task')),
    parent_id TEXT,
    project_id TEXT NOT NULL,
    position INTEGER NOT NULL DEFAULT 0,
    title TEXT NOT NULL,
    description TEXT,
    status TEXT NOT NULL DEFAULT 'backlog' CHECK(status IN ('backlog', 'todo', 'in_progress', 'review', 'done', 'blocked')),
    priority TEXT NOT NULL DEFAULT 'medium' CHECK(priority IN ('critical', 'high', 'medium', 'low')),
    story_points INTEGER,
    assignee_id TEXT,
    sprint_id TEXT,
    version INTEGER NOT NULL DEFAULT 1,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    updated_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (project_id) REFERENCES pm_projects(id) ON DELETE CASCADE,
    FOREIGN KEY (parent_id) REFERENCES pm_work_items(id) ON DELETE SET NULL,
    FOREIGN KEY (sprint_id) REFERENCES pm_sprints(id) ON DELETE SET NULL,
    FOREIGN KEY (assignee_id) REFERENCES users(id) ON DELETE SET NULL
);

-- Step 5: Recreate dependent tables WITH FK constraints
CREATE TABLE pm_comments (
    id TEXT PRIMARY KEY,
    work_item_id TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    updated_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (work_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (created_by) REFERENCES users(id),
    FOREIGN KEY (updated_by) REFERENCES users(id)
);

CREATE TABLE pm_time_entries (
    id TEXT PRIMARY KEY,
    work_item_id TEXT NOT NULL,
    user_id TEXT NOT NULL,
    started_at INTEGER NOT NULL,
    ended_at INTEGER,
    duration_seconds INTEGER,
    description TEXT,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (work_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);

CREATE TABLE pm_dependencies (
    id TEXT PRIMARY KEY,
    blocking_item_id TEXT NOT NULL,
    blocked_item_id TEXT NOT NULL,
    dependency_type TEXT NOT NULL DEFAULT 'blocks' CHECK(dependency_type IN ('blocks', 'relates_to')),
    created_at INTEGER NOT NULL,
    created_by TEXT NOT NULL,
    deleted_at INTEGER,
    FOREIGN KEY (blocking_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (blocked_item_id) REFERENCES pm_work_items(id) ON DELETE CASCADE,
    FOREIGN KEY (created_by) REFERENCES users(id),
    UNIQUE(blocking_item_id, blocked_item_id),
    CHECK(blocking_item_id != blocked_item_id)
);

-- Step 6: Restore ALL data
INSERT INTO pm_work_items (
    id, item_type, parent_id, project_id, position, title, description,
    status, priority, story_points, assignee_id, sprint_id, version,
    created_at, updated_at, created_by, updated_by, deleted_at
)
SELECT
    id, item_type, parent_id, project_id, position, title, description,
    status, priority, story_points, assignee_id, sprint_id, version,
    created_at, updated_at, created_by, updated_by, deleted_at
FROM temp_work_items;

INSERT INTO pm_comments SELECT * FROM temp_comments;
INSERT INTO pm_time_entries SELECT * FROM temp_time_entries;
INSERT INTO pm_dependencies SELECT * FROM temp_dependencies;

-- Step 7: Recreate ALL indexes
-- pm_work_items indexes (WITHOUT item_number index)
CREATE INDEX idx_pm_work_items_project ON pm_work_items(project_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_parent ON pm_work_items(parent_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_sprint ON pm_work_items(sprint_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_status ON pm_work_items(status) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_assignee ON pm_work_items(assignee_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_work_items_type ON pm_work_items(item_type) WHERE deleted_at IS NULL;

-- pm_comments indexes
CREATE INDEX idx_pm_comments_work_item ON pm_comments(work_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_comments_created ON pm_comments(created_at DESC) WHERE deleted_at IS NULL;

-- pm_time_entries indexes
CREATE INDEX idx_pm_time_entries_work_item ON pm_time_entries(work_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_user ON pm_time_entries(user_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_started ON pm_time_entries(started_at DESC) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_time_entries_running ON pm_time_entries(ended_at) WHERE ended_at IS NULL AND deleted_at IS NULL;

-- pm_dependencies indexes
CREATE INDEX idx_pm_dependencies_blocking ON pm_dependencies(blocking_item_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_pm_dependencies_blocked ON pm_dependencies(blocked_item_id) WHERE deleted_at IS NULL;

-- Step 8: Remove next_work_item_number from pm_projects
ALTER TABLE pm_projects DROP COLUMN next_work_item_number;

-- Step 9: Cleanup temp tables
DROP TABLE temp_work_items;
DROP TABLE temp_comments;
DROP TABLE temp_time_entries;
DROP TABLE temp_dependencies;

PRAGMA foreign_keys = ON;

COMMIT;
```

**When to use this:**
- Migration failed partway through and database is in inconsistent state
- Need to revert feature before next deployment
- Testing migration behavior

**Verification**: File exists at correct path

---

### Step 2: Update Protobuf Messages

**File**: `proto/messages.proto`

**Why early?** Proto generates code for both Rust (`pm-proto` crate) and C# (`ProjectManagement.Core.Proto`). Must be done before any code that references these fields.

#### 2.1 Add `item_number` to WorkItem message

Find the `message WorkItem` block and add field 19:

```protobuf
message WorkItem {
  string id = 1;
  WorkItemType item_type = 2;

  // Hierarchy
  optional string parent_id = 3;
  string project_id = 4;
  int32 position = 5;

  // Core fields
  string title = 6;
  optional string description = 7;

  // Workflow
  string status = 8;

  // Assignment
  optional string assignee_id = 9;

  // Sprint
  optional string sprint_id = 10;

  // Audit
  int64 created_at = 11;
  int64 updated_at = 12;
  string created_by = 13;
  string updated_by = 14;
  optional int64 deleted_at = 15;
  string priority = 16;
  optional int32 story_points = 17;
  int32 version = 18;
  int32 item_number = 19;  // NEW: Project-scoped sequential number
}
```

#### 2.2 Add `next_work_item_number` to Project message

Find the `message Project` block and add field 12:

```protobuf
message Project {
  string id = 1;
  string title = 2;
  optional string description = 3;
  string key = 4;
  ProjectStatus status = 5;
  int32 version = 6;
  int64 created_at = 7;
  int64 updated_at = 8;
  string created_by = 9;
  string updated_by = 10;
  optional int64 deleted_at = 11;
  int32 next_work_item_number = 12;  // NEW: Counter for work item numbers
}
```

#### 2.3 Add `parent_id` and `update_parent` to UpdateWorkItemRequest (Feature C - FIXED)

Find the `message UpdateWorkItemRequest` block and add fields 11-12:

```protobuf
message UpdateWorkItemRequest {
  string work_item_id = 1;
  int32 expected_version = 2;
  optional string title = 3;
  optional string description = 4;
  optional string status = 5;
  optional string assignee_id = 6;
  optional string sprint_id = 7;
  optional int32 position = 8;
  optional string priority = 9;
  optional int32 story_points = 10;
  optional string parent_id = 11;  // NEW (Feature C): Parent work item ID or empty to clear
  bool update_parent = 12;  // NEW (Feature C): True if parent_id should be updated
}
```

**CRITICAL FIX: Why `update_parent` field is needed**

The original plan had `UpdateParent` as a C# property only, but this doesn't reach the Rust backend! The proto needs an explicit field.

**Semantics for parent update:**
- `update_parent = false`: Don't change parent (ignore `parent_id` value)
- `update_parent = true` + `parent_id` not set or empty: Clear parent (make orphan)
- `update_parent = true` + `parent_id = UUID`: Set parent to specified work item

**Why proto3 optional is not enough:**
Proto3 `optional` allows distinguishing "field not set" from "field set to empty", but we need THREE states:
1. Don't update (no field)
2. Clear parent (empty string)
3. Set parent (UUID string)

The `update_parent` bool provides the explicit "should update" signal.

**Verification**: `cargo build -p pm-proto` succeeds

---

### Step 3: Run Database Migration

**CRITICAL:** This must be done before compiling Rust repository code. SQLx uses compile-time query checking against the actual database schema.

```bash
# Option A: If you have a dev database configured
sqlx migrate run --database-url "sqlite:///path/to/dev.db"

# Option B: Using the project's migration setup
# (Check justfile for specific command)

# Option C: If using sqlx offline mode, regenerate query cache after migration
cargo sqlx prepare --workspace
```

**Manual Verification (temporary - replaced by Step 4):**
```bash
# Confirm new columns exist
sqlite3 /path/to/dev.db ".schema pm_projects" | grep next_work_item_number
sqlite3 /path/to/dev.db ".schema pm_work_items" | grep item_number

# Confirm unique constraint exists
sqlite3 /path/to/dev.db ".schema pm_work_items" | grep "UNIQUE(project_id, item_number)"

# Confirm index exists
sqlite3 /path/to/dev.db ".indexes pm_work_items" | grep item_number
```

---

### Step 4: Add Automated Migration Tests (NEW - CRITICAL)

**File**: `backend/crates/pm-db/tests/migration_20260203000001_test.rs`

Replace manual shell commands with automated tests that run in CI.

```rust
//! Tests for migration 20260203000001_add_work_item_numbers.sql
//!
//! These tests verify that the migration:
//! 1. Adds next_work_item_number to pm_projects
//! 2. Adds item_number to pm_work_items
//! 3. Preserves existing data
//! 4. Creates proper indexes and constraints
//! 5. PRESERVES FOREIGN KEY CONSTRAINTS (CRITICAL!)

use sqlx::{SqlitePool, Row};

#[sqlx::test]
async fn test_migration_adds_project_counter(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Verify next_work_item_number column exists
    let result = sqlx::query("SELECT next_work_item_number FROM pm_projects LIMIT 1")
        .fetch_optional(&pool)
        .await;

    // Either succeeds (column exists) or table is empty (also OK)
    assert!(result.is_ok(), "next_work_item_number column should exist");

    Ok(())
}

#[sqlx::test]
async fn test_migration_adds_item_number(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Verify item_number column exists
    let result = sqlx::query("SELECT item_number FROM pm_work_items LIMIT 1")
        .fetch_optional(&pool)
        .await;

    assert!(result.is_ok(), "item_number column should exist");

    Ok(())
}

#[sqlx::test]
async fn test_migration_unique_constraint(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Insert test project
    let project_id = "test-project-123";
    sqlx::query!(
        "INSERT INTO pm_projects (id, title, key, status, version, created_at, updated_at, created_by, updated_by, next_work_item_number)
         VALUES (?, 'Test', 'TEST', 'active', 1, 0, 0, 'user-1', 'user-1', 1)",
        project_id
    )
    .execute(&pool)
    .await?;

    // Insert first work item with item_number = 1
    let work_item_1 = "work-item-1";
    sqlx::query!(
        "INSERT INTO pm_work_items (id, item_type, project_id, position, title, status, priority, item_number, version, created_at, updated_at, created_by, updated_by)
         VALUES (?, 'story', ?, 0, 'Story 1', 'backlog', 'medium', 1, 1, 0, 0, 'user-1', 'user-1')",
        work_item_1,
        project_id
    )
    .execute(&pool)
    .await?;

    // Try to insert second work item with same item_number = 1 (should fail)
    let work_item_2 = "work-item-2";
    let result = sqlx::query!(
        "INSERT INTO pm_work_items (id, item_type, project_id, position, title, status, priority, item_number, version, created_at, updated_at, created_by, updated_by)
         VALUES (?, 'story', ?, 0, 'Story 2', 'backlog', 'medium', 1, 1, 0, 0, 'user-1', 'user-1')",
        work_item_2,
        project_id
    )
    .execute(&pool)
    .await;

    // Should fail due to UNIQUE constraint
    assert!(result.is_err(), "Duplicate item_number should be rejected by UNIQUE constraint");

    Ok(())
}

#[sqlx::test]
async fn test_migration_preserves_data(pool: SqlitePool) -> sqlx::Result<()> {
    // Insert data BEFORE migration
    // (This test would need to run migration selectively - may need custom test setup)

    // For now, verify that after migration, data can be read
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Insert and retrieve a work item
    let project_id = "test-project-456";
    sqlx::query!(
        "INSERT INTO pm_projects (id, title, key, status, version, created_at, updated_at, created_by, updated_by, next_work_item_number)
         VALUES (?, 'Test', 'TEST', 'active', 1, 0, 0, 'user-1', 'user-1', 1)",
        project_id
    )
    .execute(&pool)
    .await?;

    let work_item_id = "work-item-789";
    sqlx::query!(
        "INSERT INTO pm_work_items (id, item_type, project_id, position, title, status, priority, item_number, version, created_at, updated_at, created_by, updated_by)
         VALUES (?, 'epic', ?, 0, 'Epic 1', 'backlog', 'high', 1, 1, 0, 0, 'user-1', 'user-1')",
        work_item_id,
        project_id
    )
    .execute(&pool)
    .await?;

    // Retrieve and verify
    let row = sqlx::query!("SELECT id, title, item_number FROM pm_work_items WHERE id = ?", work_item_id)
        .fetch_one(&pool)
        .await?;

    assert_eq!(row.title, "Epic 1");
    assert_eq!(row.item_number, 1);

    Ok(())
}

#[sqlx::test]
async fn test_migration_index_exists(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Check that index exists
    let indexes = sqlx::query("SELECT name FROM sqlite_master WHERE type='index' AND tbl_name='pm_work_items'")
        .fetch_all(&pool)
        .await?;

    let index_names: Vec<String> = indexes
        .iter()
        .map(|row| row.get::<String, _>("name"))
        .collect();

    assert!(
        index_names.iter().any(|name| name.contains("item_number")),
        "Index on item_number should exist"
    );

    Ok(())
}

// ============================================================
// CRITICAL FK PRESERVATION TESTS
// ============================================================

#[sqlx::test]
async fn test_migration_preserves_comments_fk(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Verify pm_comments has FK to pm_work_items
    let fk_info = sqlx::query("PRAGMA foreign_key_list(pm_comments)")
        .fetch_all(&pool)
        .await?;

    assert!(
        fk_info.iter().any(|row| row.get::<String, _>("table") == "pm_work_items"),
        "CRITICAL: pm_comments MUST have FK to pm_work_items. Migration destroyed FK constraint!"
    );

    Ok(())
}

#[sqlx::test]
async fn test_migration_preserves_time_entries_fk(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Verify pm_time_entries has FK to pm_work_items
    let fk_info = sqlx::query("PRAGMA foreign_key_list(pm_time_entries)")
        .fetch_all(&pool)
        .await?;

    assert!(
        fk_info.iter().any(|row| row.get::<String, _>("table") == "pm_work_items"),
        "CRITICAL: pm_time_entries MUST have FK to pm_work_items. Migration destroyed FK constraint!"
    );

    Ok(())
}

#[sqlx::test]
async fn test_migration_preserves_dependencies_fks(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Verify pm_dependencies has BOTH FKs to pm_work_items
    let fk_info = sqlx::query("PRAGMA foreign_key_list(pm_dependencies)")
        .fetch_all(&pool)
        .await?;

    let fk_count = fk_info.iter()
        .filter(|row| row.get::<String, _>("table") == "pm_work_items")
        .count();

    assert_eq!(
        fk_count, 2,
        "CRITICAL: pm_dependencies MUST have 2 FKs to pm_work_items (blocking_item_id and blocked_item_id). Migration destroyed FK constraints!"
    );

    Ok(())
}

#[sqlx::test]
async fn test_fk_enforcement_after_migration(pool: SqlitePool) -> sqlx::Result<()> {
    // Apply migration
    sqlx::migrate!("./migrations").run(&pool).await?;

    // Insert test project and work item
    let project_id = "test-project-fk";
    sqlx::query!(
        "INSERT INTO pm_projects (id, title, key, status, version, created_at, updated_at, created_by, updated_by, next_work_item_number)
         VALUES (?, 'Test', 'TEST', 'active', 1, 0, 0, 'user-1', 'user-1', 1)",
        project_id
    )
    .execute(&pool)
    .await?;

    let work_item_id = "work-item-fk-test";
    sqlx::query!(
        "INSERT INTO pm_work_items (id, item_type, project_id, position, title, status, priority, item_number, version, created_at, updated_at, created_by, updated_by)
         VALUES (?, 'story', ?, 0, 'Story FK Test', 'backlog', 'medium', 1, 1, 0, 0, 'user-1', 'user-1')",
        work_item_id,
        project_id
    )
    .execute(&pool)
    .await?;

    // Try to insert comment with NON-EXISTENT work_item_id
    let comment_id = "comment-bad-fk";
    let fake_work_item_id = "non-existent-work-item";
    let result = sqlx::query!(
        "INSERT INTO pm_comments (id, work_item_id, content, created_at, updated_at, created_by, updated_by)
         VALUES (?, ?, 'Test comment', 0, 0, 'user-1', 'user-1')",
        comment_id,
        fake_work_item_id
    )
    .execute(&pool)
    .await;

    // Should FAIL due to FK constraint
    assert!(
        result.is_err(),
        "CRITICAL: FK constraint violation should be enforced! Comment inserted for non-existent work item. This means FKs were destroyed by migration!"
    );

    Ok(())
}
```

**Why this matters:**
- Manual shell commands are fragile and easy to skip
- Automated tests run in CI and catch regressions
- Tests verify behavior, not just schema
- UNIQUE constraint test prevents subtle bugs

**Verification**: `cargo test -p pm-db migration_20260203000001`

---

## Session 90.1 Completion Checklist

- [ ] **Step 1**: Migration file created with:
  - [ ] Transaction wrapper
  - [ ] **FK preservation** (recreates pm_comments, pm_time_entries, pm_dependencies)
  - [ ] Saves data from ALL 4 affected tables
  - [ ] Drops dependent tables FIRST
  - [ ] Recreates all tables WITH FK constraints
  - [ ] Restores ALL data
  - [ ] Recreates ALL indexes
- [ ] **Step 1b**: Rollback migration created with **same FK preservation**
- [ ] **Step 2**: `proto/messages.proto` updated with all changes (including `update_parent` field)
- [ ] **Step 3**: Migration run successfully
- [ ] **Step 3**: Manual verification commands pass (temporary)
- [ ] **Step 4**: Automated migration tests created including:
  - [ ] Basic schema tests (columns, indexes, constraints)
  - [ ] **FK preservation tests** (CRITICAL!)
  - [ ] FK enforcement tests (verify FKs actually work)
- [ ] `cargo build -p pm-proto` succeeds (proto compiles)
- [ ] `cargo test -p pm-db migration_20260203000001` passes
- [ ] **CRITICAL**: All FK preservation tests pass

### Files Created (3)
- `backend/crates/pm-db/migrations/20260203000001_add_work_item_numbers.sql` (**FK-safe migration**)
- `backend/crates/pm-db/migrations/20260203000001_rollback_add_work_item_numbers.sql` (**FK-safe rollback**)
- `backend/crates/pm-db/tests/migration_20260203000001_test.rs` (**with FK preservation tests**)

### Files Modified (1)
- `proto/messages.proto` (added `item_number`, `next_work_item_number`, `parent_id`, **`update_parent`**)

---

## Next Session

**Session 90.2** will implement the Rust backend:
- Domain models with new fields
- Repository methods including atomic counter
- Response builder updates
- Work item handlers
- Circular reference prevention
